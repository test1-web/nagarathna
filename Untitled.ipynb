{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8f71f29-c4ff-4a39-ac7d-e80017752c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (891, 11)\n",
      "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
      "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
      "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
      "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
      "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
      "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
      "\n",
      "     who  alone  \n",
      "0    man  False  \n",
      "1  woman  False  \n",
      "2  woman   True  \n",
      "3  woman  False  \n",
      "4    man   True  \n",
      "\n",
      "Target distribution:\n",
      "survived\n",
      "0    0.616162\n",
      "1    0.383838\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Train/val shape: (712, 10), Test shape: (179, 10)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 122\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Pipelines for each type\u001b[39;00m\n\u001b[0;32m    110\u001b[0m numeric_transformer \u001b[38;5;241m=\u001b[39m Pipeline(\n\u001b[0;32m    111\u001b[0m     steps\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    112\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m\"\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m    113\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m\"\u001b[39m, StandardScaler()),\n\u001b[0;32m    114\u001b[0m     ]\n\u001b[0;32m    115\u001b[0m )\n\u001b[0;32m    117\u001b[0m categorical_transformer \u001b[38;5;241m=\u001b[39m Pipeline(\n\u001b[0;32m    118\u001b[0m     steps\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    119\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m\"\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m    120\u001b[0m         (\n\u001b[0;32m    121\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monehot\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 122\u001b[0m             OneHotEncoder(handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m    123\u001b[0m         ),  \u001b[38;5;66;03m# sparse=False to easily inspect feature names\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     ]\n\u001b[0;32m    125\u001b[0m )\n\u001b[0;32m    127\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer(\n\u001b[0;32m    128\u001b[0m     transformers\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    129\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m\"\u001b[39m, numeric_transformer, numeric_features),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m     remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    133\u001b[0m )\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# 4) Baseline models (pipeline-wrapped)\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "# Full end-to-end ML workflow using the Titanic dataset (classification)\n",
    "# Run in Jupyter or plain Python. Requires: pandas, numpy, scikit-learn, seaborn, matplotlib, joblib\n",
    "# Optional: shap for feature explanations\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    "    RandomizedSearchCV,\n",
    "    learning_curve,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "import matplotlib.ticker as mtick\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 1) Load dataset & quick EDA\n",
    "# -----------------------\n",
    "def load_and_clean_titanic():\n",
    "    df = sns.load_dataset(\"titanic\")  # seaborn dataset (mixed types)\n",
    "    # Keep a subset of sensible features for tabular demo\n",
    "    df = df[\n",
    "        [\n",
    "            \"survived\",  # target\n",
    "            \"pclass\",\n",
    "            \"sex\",\n",
    "            \"age\",\n",
    "            \"sibsp\",\n",
    "            \"parch\",\n",
    "            \"fare\",\n",
    "            \"embarked\",\n",
    "            \"class\",  # duplicate of pclass but string\n",
    "            \"who\",\n",
    "            \"alone\",\n",
    "        ]\n",
    "    ].copy()\n",
    "\n",
    "    # Drop rows where target missing\n",
    "    df = df[df[\"survived\"].notna()].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_and_clean_titanic()\n",
    "print(\"shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# Quick target balance\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(df[\"survived\"].value_counts(normalize=True))\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 2) Train/val/test split (stratified)\n",
    "# -----------------------\n",
    "TARGET = \"survived\"\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET].astype(int)\n",
    "\n",
    "# First: test holdout 20%, then during CV we'll use train for tuning.\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Optional: further split train/val in code if you want a separate validation set.\n",
    "print(f\"\\nTrain/val shape: {X_trainval.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 3) Preprocessing pipeline\n",
    "# -----------------------\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = [\"age\", \"sibsp\", \"parch\", \"fare\"]\n",
    "categorical_features = [\"pclass\", \"sex\", \"embarked\", \"class\", \"who\", \"alone\"]\n",
    "\n",
    "# Pipelines for each type\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\n",
    "            \"onehot\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\", sparse=False),\n",
    "        ),  # sparse=False to easily inspect feature names\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 4) Baseline models (pipeline-wrapped)\n",
    "# -----------------------\n",
    "def make_pipeline(model):\n",
    "    return Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", model)])\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"logreg\": make_pipeline(\n",
    "        LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
    "    ),\n",
    "    \"rf\": make_pipeline(\n",
    "        RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Quick cross-validated baseline comparison (5-fold stratified)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "print(\"\\nBaseline cross-validated ROC-AUC scores:\")\n",
    "for name, pipe in models.items():\n",
    "    scores = cross_val_score(\n",
    "        pipe, X_trainval, y_trainval, cv=cv, scoring=\"roc_auc\", n_jobs=-1\n",
    "    )\n",
    "    print(f\"  {name}: mean ROC-AUC = {scores.mean():.4f} (std {scores.std():.4f})\")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 5) Hyperparameter tuning for the chosen model (use RandomForest here)\n",
    "# -----------------------\n",
    "# Search space for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    \"clf__n_estimators\": [50, 100, 200, 400],\n",
    "    \"clf__max_depth\": [None, 5, 8, 12, 20],\n",
    "    \"clf__min_samples_split\": [2, 5, 10],\n",
    "    \"clf__min_samples_leaf\": [1, 2, 4],\n",
    "    \"clf__max_features\": [\"sqrt\", \"log2\", 0.5, None],\n",
    "    \"clf__class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "n_iter_search = 40\n",
    "rs = RandomizedSearchCV(\n",
    "    models[\"rf\"],\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=n_iter_search,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"\\nStarting RandomizedSearchCV for RandomForest...\")\n",
    "t0 = time.time()\n",
    "rs.fit(X_trainval, y_trainval)\n",
    "t1 = time.time()\n",
    "print(f\"Random search done in {t1-t0:.1f}s\")\n",
    "print(\"Best params:\", rs.best_params_)\n",
    "print(f\"Best CV ROC-AUC: {rs.best_score_:.4f}\")\n",
    "\n",
    "best_model = rs.best_estimator_  # pipeline with preprocessor + best RF\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 6) Evaluate final model on holdout test set\n",
    "# -----------------------\n",
    "def evaluate_model(pipeline, X_test, y_test, display_plots=True):\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    if hasattr(pipeline, \"predict_proba\"):\n",
    "        y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        # Some models might not have predict_proba; use decision_function if available\n",
    "        try:\n",
    "            y_proba = pipeline.decision_function(X_test)\n",
    "            y_proba = (y_proba - y_proba.min()) / (y_proba.max() - y_proba.min())\n",
    "        except Exception:\n",
    "            y_proba = None\n",
    "\n",
    "    results = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
    "    }\n",
    "    if y_proba is not None:\n",
    "        results[\"roc_auc\"] = roc_auc_score(y_test, y_proba)\n",
    "        results[\"pr_auc\"] = average_precision_score(y_test, y_proba)\n",
    "    else:\n",
    "        results[\"roc_auc\"] = None\n",
    "        results[\"pr_auc\"] = None\n",
    "\n",
    "    print(\"\\nTest set metrics:\")\n",
    "    for k, v in results.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "    if display_plots and y_proba is not None:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        # ROC\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "        axs[0].plot(fpr, tpr, label=f\"AUC = {results['roc_auc']:.3f}\")\n",
    "        axs[0].plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "        axs[0].set_title(\"ROC Curve\")\n",
    "        axs[0].set_xlabel(\"False Positive Rate\")\n",
    "        axs[0].set_ylabel(\"True Positive Rate\")\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Precision-Recall\n",
    "        prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "        axs[1].plot(rec, prec, label=f\"PR AUC = {results['pr_auc']:.3f}\")\n",
    "        axs[1].set_title(\"Precision-Recall Curve\")\n",
    "        axs[1].set_xlabel(\"Recall\")\n",
    "        axs[1].set_ylabel(\"Precision\")\n",
    "        axs[1].legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return results, y_pred, (y_proba if 'y_proba' in locals() else None)\n",
    "\n",
    "\n",
    "test_results, y_pred_test, y_proba_test = evaluate_model(best_model, X_test, y_test)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 7) Feature importance inspection (for tree models)\n",
    "# -----------------------\n",
    "def get_feature_names(preprocessor):\n",
    "    # numeric features first\n",
    "    num_feats = numeric_features\n",
    "    # for onehot, get categories\n",
    "    ohe = preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "    cat_cols = []\n",
    "    try:\n",
    "        # sklearn >= 1.0\n",
    "        ohe_feature_names = ohe.get_feature_names_out(categorical_features)\n",
    "    except Exception:\n",
    "        # fallback\n",
    "        ohe_feature_names = []\n",
    "    return list(num_feats) + list(ohe_feature_names)\n",
    "\n",
    "\n",
    "try:\n",
    "    # extract feature importances if classifier supports it\n",
    "    clf = best_model.named_steps[\"clf\"]\n",
    "    if hasattr(clf, \"feature_importances_\"):\n",
    "        feat_names = get_feature_names(best_model.named_steps[\"preprocessor\"])\n",
    "        importances = clf.feature_importances_\n",
    "        imp_df = pd.DataFrame({\"feature\": feat_names, \"importance\": importances})\n",
    "        imp_df = imp_df.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "        print(\"\\nTop feature importances:\")\n",
    "        print(imp_df.head(10))\n",
    "        # plot\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.barh(imp_df[\"feature\"].head(10)[::-1], imp_df[\"importance\"].head(10)[::-1])\n",
    "        plt.title(\"Top 10 feature importances\")\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Model does not provide feature_importances_.\")\n",
    "except Exception as e:\n",
    "    print(\"Could not compute feature importances:\", e)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 8) Learning curve to check under/overfitting\n",
    "# -----------------------\n",
    "def plot_learning_curve(estimator, X, y, cv, scoring=\"roc_auc\", n_jobs=-1):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, scoring=scoring, n_jobs=n_jobs, train_sizes=np.linspace(0.1, 1.0, 5), random_state=RANDOM_STATE\n",
    "    )\n",
    "    train_mean = train_scores.mean(axis=1)\n",
    "    test_mean = test_scores.mean(axis=1)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_sizes, train_mean, \"o-\", label=\"Train\")\n",
    "    plt.plot(train_sizes, test_mean, \"o-\", label=\"CV\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(scoring)\n",
    "    plt.title(\"Learning curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curve(best_model, X_trainval, y_trainval, cv=cv, scoring=\"roc_auc\")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 9) Optional: SHAP explanations (if installed)\n",
    "# -----------------------\n",
    "try:\n",
    "    import shap\n",
    "\n",
    "    print(\"\\nRunning SHAP explainability (can be slow) ...\")\n",
    "    # Use TreeExplainer for tree models\n",
    "    # Need raw feature matrix after preprocessing to match model input:\n",
    "    pre = best_model.named_steps[\"preprocessor\"]\n",
    "    X_test_transformed = pre.transform(X_test)\n",
    "    # If pipeline used OneHotEncoder with sparse=False, transform returns numpy array\n",
    "    explainer = shap.TreeExplainer(best_model.named_steps[\"clf\"])\n",
    "    shap_values = explainer.shap_values(X_test_transformed)\n",
    "    # get feature names\n",
    "    feat_names = get_feature_names(pre)\n",
    "    # summary plot (class 1)\n",
    "    shap.summary_plot(shap_values[1], X_test_transformed, feature_names=feat_names, show=True)\n",
    "except Exception as e:\n",
    "    print(\"SHAP not run (missing package or other issue). To enable: pip install shap. Error:\", e)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 10) Save the final model pipeline\n",
    "# -----------------------\n",
    "outdir = Path(\"models\")\n",
    "outdir.mkdir(exist_ok=True)\n",
    "timestamp = int(time.time())\n",
    "model_path = outdir / f\"titanic_rf_pipeline_{timestamp}.joblib\"\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"\\nSaved best pipeline to: {model_path}\")\n",
    "\n",
    "# -----------------------\n",
    "# 11) Example: loading the pipeline and predicting on new data\n",
    "# -----------------------\n",
    "loaded = joblib.load(model_path)\n",
    "sample = X_test.sample(3, random_state=RANDOM_STATE)\n",
    "print(\"\\nSample rows and predicted probabilities:\")\n",
    "print(sample)\n",
    "print(\"Pred probs:\", loaded.predict_proba(sample)[:, 1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
